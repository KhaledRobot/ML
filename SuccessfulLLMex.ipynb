{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KhaledRobot/ML/blob/main/SuccessfulLLMex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gK6SxngU_RK6",
        "outputId": "6296a518-f029-45d6-8684-b11b88eab8b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.75.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n"
          ]
        }
      ],
      "source": [
        "pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UtDsPY57CL7X",
        "outputId": "7143b38e-fd90-4d88-e304-aef7fc62018b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.75.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HB8TdRBTY1n",
        "outputId": "5b64b595-7489-46a2-f7a8-6a11cacf230d",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.22.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.11.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.0)\n",
            "Downloading groq-0.22.0-py3-none-any.whl (126 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.7/126.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.22.0\n"
          ]
        }
      ],
      "source": [
        "pip install groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import csv\n",
        "import torch\n",
        "import time\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from groq import Groq, RateLimitError\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import random\n",
        "\n",
        "# Initialize GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.backends.cuda.max_split_size_mb = 128\n",
        "\n",
        "# ==== FULL SYSTEM PROMPTS ====\n",
        "DEFENSE_SYSTEM = \"\"\"You are an experienced defense attorney. Your role is to defend your client against the charges by:\n",
        "1. Challenging the prosecution's evidence\n",
        "2. Highlighting reasonable doubt\n",
        "3. Protecting your client's rights\n",
        "4. Providing alternative explanations when possible\n",
        "5. Being persuasive but ethical\n",
        "\n",
        "You specialize in criminal law and know courtroom procedures perfectly.\"\"\"\n",
        "PROSECUTION_SYSTEM = \"\"\"You are a seasoned prosecutor. Your duties include:\n",
        "1. Presenting the case against the defendant clearly\n",
        "2. Establishing guilt beyond reasonable doubt\n",
        "3. Countering defense arguments effectively\n",
        "4. Ensuring justice is served\n",
        "5. Maintaining professional ethics\n",
        "\n",
        "You're an expert in criminal law and courtroom tactics.\"\"\"\n",
        "PLAINTIFF_SYSTEM = \"\"\"You are the plaintiff in this case. Your role is to:\n",
        "1. Describe what happened truthfully\n",
        "2. Explain how you were harmed\n",
        "3. Answer questions clearly without speculation\n",
        "4. Stick to facts you personally witnessed\n",
        "5. Avoid emotional outbursts\"\"\"\n",
        "WITNESS_SYSTEM = \"\"\"You are an eyewitness testifying in court. Your responsibilities:\n",
        "1. Answer only what you directly observed\n",
        "2. Be precise about what you saw/heard\n",
        "3. Admit when you don't remember\n",
        "4. Don't speculate or guess\n",
        "5. Remain calm under questioning\"\"\"\n",
        "DEFENDANT_SYSTEM = \"\"\"You are the defendant in this criminal case. When speaking:\n",
        "1. Maintain your presumption of innocence\n",
        "2. Answer truthfully but carefully\n",
        "3. Consult with your attorney when needed\n",
        "4. Don't volunteer unnecessary information\n",
        "5. Behave respectfully in court\"\"\"\n",
        "JUDGE_SYSTEM = \"\"\"You are the presiding judge. Your duties:\n",
        "1. Ensure proper courtroom procedure\n",
        "2. Remain completely impartial\n",
        "3. Evaluate evidence objectively\n",
        "4. Apply the law correctly\n",
        "5. Deliver a reasoned verdict\n",
        "\n",
        "You have 20 years of judicial experience and deep legal knowledge.\"\"\"\n",
        "# Initialize models\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
        "sentence_model = SentenceTransformer('all-mpnet-base-v2').to(device)\n",
        "\n",
        "# Configure Groq client with retry logic\n",
        "class GroqClientWithRetry:\n",
        "    def __init__(self, api_key):\n",
        "        self.client = Groq(api_key=api_key)\n",
        "        self.last_call_time = 0\n",
        "\n",
        "    def create_chat_completion(self, **kwargs):\n",
        "        current_time = time.time()\n",
        "        elapsed = current_time - self.last_call_time\n",
        "        min_delay = 1.5  # Minimum delay between calls (1.5s)\n",
        "\n",
        "        if elapsed < min_delay:\n",
        "            sleep_time = min_delay - elapsed + random.uniform(0, 0.5)\n",
        "            time.sleep(sleep_time)\n",
        "\n",
        "        attempts = 0\n",
        "        max_attempts = 5\n",
        "        base_delay = 1.5\n",
        "\n",
        "        while attempts < max_attempts:\n",
        "            try:\n",
        "                self.last_call_time = time.time()\n",
        "                return self.client.chat.completions.create(**kwargs)\n",
        "            except RateLimitError as e:\n",
        "                attempts += 1\n",
        "                if attempts == max_attempts:\n",
        "                    raise\n",
        "\n",
        "                # Exponential backoff with jitter\n",
        "                delay = base_delay * (2 ** attempts) + random.uniform(0, 1)\n",
        "                time.sleep(delay)\n",
        "            except Exception as e:\n",
        "                raise e\n",
        "\n",
        "groq_client = GroqClientWithRetry(\"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\")\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"cases.csv\", usecols=[\"id\", \"text\"])\n",
        "cases = df.to_dict('records')\n",
        "\n",
        "# GPU-accelerated processing\n",
        "def gpu_process_text(text):\n",
        "    inputs = tokenizer(text[:10000], return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "# Case processing with robust error handling\n",
        "def process_case(case):\n",
        "    try:\n",
        "        # GPU processing\n",
        "        with torch.cuda.amp.autocast():\n",
        "            embedding = gpu_process_text(case['text'])\n",
        "            case['embedding'] = embedding.cpu().numpy()\n",
        "\n",
        "        # Summarization with retry\n",
        "        summary = groq_client.create_chat_completion(\n",
        "            model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"Provide a comprehensive legal case summary...\"},\n",
        "                {\"role\": \"user\", \"content\": case['text'][:15000]}\n",
        "            ],\n",
        "            temperature=0.3,\n",
        "            max_tokens=400\n",
        "        ).choices[0].message.content.strip()\n",
        "        case['summary'] = summary\n",
        "\n",
        "        # Trial simulation\n",
        "        agents = initialize_agents()\n",
        "        verdict = run_trial_simulation(agents, summary)\n",
        "\n",
        "        label = 1 if \"not guilty\" in verdict.lower() else 0\n",
        "        return (case['id'], label, \"success\")\n",
        "\n",
        "    except RateLimitError as e:\n",
        "        return (case['id'], -1, f\"rate_limit_error: {str(e)}\")\n",
        "    except Exception as e:\n",
        "        return (case['id'], -1, f\"processing_error: {str(e)}\")\n",
        "\n",
        "# Initialize agents with rate limit awareness\n",
        "class LawyerAgent:\n",
        "    def __init__(self, name, system_prompt):\n",
        "        self.name = name\n",
        "        self.system_prompt = system_prompt\n",
        "        self.history = []\n",
        "\n",
        "    def respond(self, prompt, client):\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": self.system_prompt},\n",
        "            *self.history[-4:],\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "        response = client.create_chat_completion(\n",
        "            model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
        "            messages=messages,\n",
        "            temperature=0.7,\n",
        "            max_tokens=256\n",
        "        )\n",
        "        reply = response.choices[0].message.content.strip()\n",
        "        self.history.extend([\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "            {\"role\": \"assistant\", \"content\": reply}\n",
        "        ])\n",
        "        return reply\n",
        "\n",
        "def run_trial_simulation(agents, case_summary):\n",
        "    transcript = []\n",
        "    client = GroqClientWithRetry(\"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\")\n",
        "\n",
        "    # Opening statements\n",
        "    prosecution_open = agents[\"prosecution\"].respond(f\"Opening statement. Case: {case_summary}\", client)\n",
        "    defense_open = agents[\"defense\"].respond(\"Opening statement responding to prosecution\", client)\n",
        "    transcript.extend([prosecution_open, defense_open])\n",
        "    time.sleep(1.5)  # Rate limit buffer\n",
        "\n",
        "    # Examination\n",
        "    prose_q = agents[\"prosecution\"].respond(\"Question the witness\", client)\n",
        "    witness_a = agents[\"witness\"].respond(prose_q, client)\n",
        "    transcript.extend([prose_q, witness_a])\n",
        "    time.sleep(1.5)\n",
        "\n",
        "    # Cross-examination\n",
        "    defense_q = agents[\"defense\"].respond(\"Cross-examine the witness\", client)\n",
        "    witness_a2 = agents[\"witness\"].respond(defense_q, client)\n",
        "    transcript.extend([defense_q, witness_a2])\n",
        "    time.sleep(1.5)\n",
        "\n",
        "    # Closing arguments\n",
        "    prose_close = agents[\"prosecution\"].respond(\"Closing argument\", client)\n",
        "    defense_close = agents[\"defense\"].respond(\"Closing argument\", client)\n",
        "    transcript.extend([prose_close, defense_close])\n",
        "    time.sleep(1.5)\n",
        "\n",
        "    # Verdict\n",
        "    verdict = agents[\"judge\"].respond(\n",
        "        f\"Based on this trial:\\n{case_summary[:5000]}\\n\\n\"\n",
        "        \"Key exchanges:\\n\" + \"\\n\".join(transcript[-4:]) +\n",
        "        \"\\n\\nRender verdict ('guilty' or 'not guilty') with reasoning.\",\n",
        "        client\n",
        "    )\n",
        "    return verdict\n",
        "\n",
        "# Main processing with enhanced rate control\n",
        "def process_cases_parallel(cases, max_workers=3, cases_per_batch=5):\n",
        "    results = []\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        futures = []\n",
        "\n",
        "        for i in range(0, min(50, len(cases)), cases_per_batch):\n",
        "            batch = cases[i:i + cases_per_batch]\n",
        "\n",
        "            # Submit batch with delay between submissions\n",
        "            for case in batch:\n",
        "                futures.append(executor.submit(process_case, case))\n",
        "                time.sleep(0.5)  # Stagger submissions\n",
        "\n",
        "            # Monitor progress\n",
        "            print(f\"Submitted batch {i//cases_per_batch + 1}, GPU Mem: {torch.cuda.memory_allocated()/1024**3:.2f}GB\")\n",
        "\n",
        "        # Process results as they complete\n",
        "        for future in as_completed(futures):\n",
        "            case_id, label, status = future.result()\n",
        "            results.append((case_id, label, status))\n",
        "\n",
        "            # Update monitoring\n",
        "            if \"error\" in status:\n",
        "                print(f\"Case {case_id} failed: {status}\")\n",
        "            else:\n",
        "                print(f\"Case {case_id} completed successfully\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Run processing\n",
        "with open(\"verdicts.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"id\", \"label\", \"status\"])\n",
        "\n",
        "    results = process_cases_parallel(cases)\n",
        "    for case_id, label, status in results:\n",
        "        writer.writerow([case_id, label, status])\n",
        "        f.flush()\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "print(\"Processing complete. Results saved to verdicts.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DZb0LMS8PEn0",
        "outputId": "91239be4-490c-4ab7-a0b6-d4b31c8bbcb7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-80b05206739e>:115: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submitted batch 1, GPU Mem: 1.68GB\n",
            "Submitted batch 2, GPU Mem: 1.68GB\n",
            "Submitted batch 3, GPU Mem: 1.68GB\n",
            "Submitted batch 4, GPU Mem: 1.68GB\n",
            "Submitted batch 5, GPU Mem: 1.68GB\n",
            "Submitted batch 6, GPU Mem: 1.68GB\n",
            "Submitted batch 7, GPU Mem: 1.68GB\n",
            "Submitted batch 8, GPU Mem: 1.68GB\n",
            "Submitted batch 9, GPU Mem: 1.68GB\n",
            "Submitted batch 10, GPU Mem: 1.68GB\n",
            "Case 1989_75 completed successfully\n",
            "Case 1959_102 completed successfully\n",
            "Case 1979_76 completed successfully\n",
            "Case 1984_73 completed successfully\n",
            "Case 1985_233 completed successfully\n",
            "Case 1991_124 completed successfully\n",
            "Case 1963_191 completed successfully\n",
            "Case 1992_1 completed successfully\n",
            "Case 1963_285 completed successfully\n",
            "Case 1961_166 completed successfully\n",
            "Case 1990_342 completed successfully\n",
            "Case 1975_183 completed successfully\n",
            "Case 1962_237 completed successfully\n",
            "Case 1985_23 completed successfully\n",
            "Case 1971_570 completed successfully\n",
            "Case 1984_63 completed successfully\n",
            "Case 1988_248 completed successfully\n",
            "Case 1988_201 completed successfully\n",
            "Case 1967_162 completed successfully\n",
            "Case 1958_67 completed successfully\n",
            "Case 1954_181 completed successfully\n",
            "Case 1983_89 completed successfully\n",
            "Case 1971_338 completed successfully\n",
            "Case 1969_369 completed successfully\n",
            "Case 1977_0 completed successfully\n",
            "Case 1971_138 completed successfully\n",
            "Case 1983_169 completed successfully\n",
            "Case 1977_200 completed successfully\n",
            "Case 1977_103 completed successfully\n",
            "Case 1960_229 completed successfully\n",
            "Case 1968_179 completed successfully\n",
            "Case 1968_350 completed successfully\n",
            "Case 1966_153 completed successfully\n",
            "Case 1976_40 completed successfully\n",
            "Case 1961_431 completed successfully\n",
            "Case 1979_446 completed successfully\n",
            "Case 1961_123 completed successfully\n",
            "Case 1980_164 completed successfully\n",
            "Case 1974_386 completed successfully\n",
            "Case 1977_120 completed successfully\n",
            "Case 1978_306 completed successfully\n",
            "Case 1958_49 completed successfully\n",
            "Case 1974_86 completed successfully\n",
            "Case 1991_142 completed successfully\n",
            "Case 1966_269 completed successfully\n",
            "Case 1975_38 completed successfully\n",
            "Case 1965_327 completed successfully\n",
            "Case 1986_422 completed successfully\n",
            "Case 1982_6 completed successfully\n",
            "Case 1968_86 completed successfully\n",
            "Processing complete. Results saved to verdicts.csv\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}