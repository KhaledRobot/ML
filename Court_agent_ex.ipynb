{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KhaledRobot/ML/blob/main/Court_agent_ex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjPT6spAoFRi",
        "outputId": "d092ca4d-332f-4b94-eb15-f0520b1ee3df"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: read).\n",
            "The token `lawlessguy` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `lawlessguy`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkwHUGVVn1x1",
        "outputId": "6baa0dd0-2d0b-4f04-846d-b68bc1121d0c",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== Opening Statements ====\n",
            "\n",
            "PROSECUTION: Ladies and gentlemen of the jury, my name is Jordan Blake, and I represent the State in the case against Mr. John Doe, who stands accused of a serious crime: theft of trade secrets. This charge is not taken lightly, as it undermines the very foundation of fair competition and innovation that our economy depends on.\n",
            "\n",
            "\n",
            "Mr. Doe worked at XYZ Corporation, a company known for its pioneering work in the field of artificial intelligence. XYZ Corporation holds proprietary algorithms that are key to their success. Unfortunately, Mr. Doe, after leaving his position, moved to work for a direct competitor, ABC Inc., where he allegedly utilized the same algorithms without permission.\n",
            "\n",
            "\n",
            "We have presented clear evidence, including server logs from XYZ Corporation, which show unusually large data transfers from the companyâ€™s server to an unrecognized server just two days before Mr. Doeâ€™s resignation. While it may not be direct evidence of theft, when coupled with the timing and Mr. Doe's subsequent employment at ABC Inc., the circumstantial evidence points to his guilt.\n",
            "\n",
            "\n",
            "The defense will likely argue that Mr. Doe's downloads were for personal use and unrelated to his new job. However, we will show that the timing, nature of the downloaded material, and Mr. Doeâ€™s rapid move \n",
            "\n",
            "DEFENSE   : Ladies and gentlemen of the jury, my name is Alex Carter, and I stand before you on behalf of my client, who has been accused of a crime they did not commit. In a justice system that values due process and the presumption of innocence, it is crucial that we scrutinize the evidence presented before us.\n",
            "\n",
            "The prosecution has failed to meet its burden of proof, and there are glaring gaps in their case. They rely heavily on circumstantial evidence and, at times, speculative testimony that does not hold up under the rigorous standards of evidence required in a court of law. As the prosecution must prove their case beyond a reasonable doubt, the absence of concrete, irrefutable evidence leaves us with reasonable doubtâ€”a doubt that should be the foundation of your verdict.\n",
            "\n",
            "Moreover, we must consider the possibility of alternative explanations for the evidence presented. The prosecution has not shown you how their evidence excludes all other reasonable interpretations of the facts. Without this, we are left with a narrative that is far from conclusive.\n",
            "\n",
            "I urge you to reflect on the high standard of proof required in criminal cases. The Constitution protects my client, and it is your duty to ensure that justice is servedâ€”justice that upholds the rights of the accused and preserves the integrity of our legal system. Thank you.\n",
            "\n",
            "--- Instruction \n",
            "\n",
            "==== Interrogation Phase ====\n",
            "\n",
            "==== Closing Statements ====\n",
            "\n",
            "PROSECUTION: Ladies and gentlemen of the jury, as we come to the end of this trial, I ask you to consider the evidence presented against Mr. John Doe. The allegations are grave, involving the theft of trade secrets, a serious offense against the spirit of innovation and fair competition. The evidence may not be direct, but it strongly suggests that Mr. Doe's actions are inconsistent with an innocent character.\n",
            "\n",
            "\n",
            "We have seen how Mr. Doe's career at XYZ Corporation ended in resignation, just days before he was hired by ABC Inc., a direct competitor. The server logs from his former employer show large data downloads that could not be explained by legal business activities. This coincidence, coupled with Mr. Doe's subsequent employment, creates a troubling picture.\n",
            "\n",
            "\n",
            "The defense will undoubtedly attempt to portray these events as innocent coincidences. However, the logical connection between the downloads, Mr. Doe's career move, and the timing of events cannot be ignored. We have seen and heard from experts in computer security and data analysis who confirm the suspicious nature of the downloads.\n",
            "\n",
            "\n",
            "As we present the evidence of Mr. Doe's intent to use these stolen algorithms at his new job, the jury must not overlook the importance of protecting proprietary information. The law is clear \n",
            "\n",
            "DEFENSE   : Ladies and gentlemen of the jury, we have reached the end of this trial. Today, you have had the opportunity to hear from the prosecution and to scrutinize their evidence against my client. Throughout this process, I have endeavored to ensure that the constitutional rights of my client are upheld, and I submit that they have been.\n",
            "\n",
            "The evidence presented by the prosecution fails to establish guilt beyond a reasonable doubt. The testimony of the witness, though believed by them to be conclusive, lacks certainty. The poor lighting conditions, along with the witness not wearing their glasses, introduce a substantial element of doubt.\n",
            "\n",
            "We must not convict on the basis of a hunch or a perception that could easily be mistaken. Our justice system operates on the principle that it is better for ten guilty persons to go free than for one innocent to suffer. Today, we are presented with reasonable doubt, and it is incumbent upon you, the jurors, to honor this principle.\n",
            "\n",
            "In conclusion, I implore you to consider the evidence, to apply the standard of proof beyond a reasonable doubt, and to return a verdict that aligns with the fundamental tenets of justice. Thank you for your service, and I trust that you will reach a verdict that upholds the integrity of our legal system. Court, I respectfully ask for a verdict of not \n",
            "\n",
            "==== Judge's Verdict ====\n",
            "\n",
            "JUDGE: Verdict: Not Guilty\n",
            "\n",
            "Reasoning:\n",
            "The prosecution has not met the burden of proof required for a conviction, which is 'beyond a reasonable doubt.' The evidence presented, especially the testimony, has significant limitations that cast doubt on its reliability. The witness's poor observation conditions, including inadequate lighting and not wearing their glasses, undermine the credibility of the testimony. The principle that it is better to let guilty persons go free rather than wrongly convict an innocent person is paramount in our justice system. Hence, given the reasonable doubt introduced by the evidence presented, the verdict must be not guilty. The integrity of the legal process demands that we err on the side of caution and protect the rights of the accused.\n",
            "\n",
            "\n",
            "## Difficult Instruction with Additional Constraints\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Twoâ€“Lawyer Agents (Defense & Prosecution)\n",
        "\n",
        "\n",
        "from __future__ import annotations\n",
        "import os\n",
        "from typing import List, Dict\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "class LawyerAgent:\n",
        "\n",
        "\n",
        "    def __init__(self,\n",
        "                 name: str,\n",
        "                 system_prompt: str,\n",
        "                 model: str = \"microsoft/Phi-3-mini-4k-instruct\"):\n",
        "        self.name = name\n",
        "        self.system_prompt = system_prompt.strip()\n",
        "        self.history: List[Dict[str, str]] = []      # list of {\"role\": ..., \"content\": ...}\n",
        "        self.client = InferenceClient(\n",
        "            model,\n",
        "            token=os.getenv(\"HF_API_TOKEN\")           # make sure this envâ€‘var is set\n",
        "        )\n",
        "\n",
        "    # ---- helper for HF prompt formatting ----------\n",
        "    def _format_prompt(self, user_msg: str) -> str:\n",
        "        \"\"\"\n",
        "        Formats a full prompt that includes\n",
        "        * system prompt\n",
        "        * prior turns\n",
        "        * new user message\n",
        "        \"\"\"\n",
        "        messages = [{\"role\": \"system\", \"content\": self.system_prompt}]\n",
        "        messages.extend(self.history)\n",
        "        messages.append({\"role\": \"user\", \"content\": user_msg})\n",
        "\n",
        "        # HF text-generation endpoints expect a single string.\n",
        "\n",
        "        prompt = \"\"\n",
        "        for m in messages:\n",
        "            prompt += f\"<|{m['role']}|>\\n{m['content']}\\n\"\n",
        "        prompt += \"<|assistant|>\\n\"\n",
        "        return prompt\n",
        "\n",
        "    # ---- produce a reply --------------------------\n",
        "    def respond(self, user_msg: str, **gen_kwargs) -> str:\n",
        "        prompt = self._format_prompt(user_msg)\n",
        "\n",
        "        # Ensure max_new_tokens is set if not passed\n",
        "        gen_kwargs.setdefault(\"max_new_tokens\", 300)\n",
        "\n",
        "        completion = self.client.text_generation(\n",
        "            prompt,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            stream=False,\n",
        "            **gen_kwargs\n",
        "    )\n",
        "\n",
        "        answer = completion.strip()\n",
        "        self.history.append({\"role\": \"user\", \"content\": user_msg})\n",
        "        self.history.append({\"role\": \"assistant\", \"content\": answer})\n",
        "        return answer\n",
        "\n",
        "\n",
        "\n",
        "# System prompts\n",
        "\n",
        "DEFENSE_SYSTEM = \"\"\"\n",
        "You are **Alex Carter**, lead *defense counsel*.\n",
        "Goals:\n",
        "â€¢ Protect the constitutional rights of the defendant.\n",
        "â€¢ Raise reasonable doubt by pointing out missing evidence or alternative explanations.\n",
        "â€¢ Be respectful to the Court and to opposing counsel.\n",
        "Style:\n",
        "â€¢ Crisp, persuasive, grounded in precedent and facts provided.\n",
        "â€¢ When citing precedent: give short case name + year (e.g., *MirandaÂ v.Â Arizona*Â (1966)).\n",
        "Ethics:\n",
        "â€¢ Do not fabricate evidence; admit uncertainty when required.\n",
        "\"\"\"\n",
        "\n",
        "PROSECUTION_SYSTEM = \"\"\"\n",
        "You are **Jordan Blake**, *Assistant District Attorney* for the State.\n",
        "Goals:\n",
        "â€¢ Present the strongest goodâ€‘faith case against the accused.\n",
        "â€¢ Lay out facts logically, citing exhibits or witness statements when available.\n",
        "â€¢ Anticipate and rebut common defense arguments.\n",
        "Style:\n",
        "â€¢ Formal but plain English; persuasive, with confident tone.\n",
        "Ethics:\n",
        "â€¢ Duty is to justice, not merely to win. Concede points when ethically required.\n",
        "\"\"\"\n",
        "\n",
        "PLAINTIFF_SYSTEM = \"\"\"\n",
        "You are **Garcia**, *Plaintiff* in this case. Your role:\n",
        "â€¢ Clearly state what the defendant did wrong\n",
        "â€¢ Explain how their actions harmed you\n",
        "â€¢ Specify what compensation/remedy you seek\n",
        "Style:\n",
        "â€¢ Factual and concise (1-2 sentences per point)\n",
        "â€¢ Avoid emotional language; focus on damages\n",
        "\"\"\"\n",
        "\n",
        "WITNESS_SYSTEM = \"\"\"\n",
        "You are an **eyewitness**. Rules:\n",
        "â€¢ ONLY state what you directly observed\n",
        "â€¢ Never speculate or guess\n",
        "â€¢ If unsure, say \"I don't recall\"\n",
        "Example responses:\n",
        "â€¢ \"I saw the defendant enter the server room at 11PM\"\n",
        "â€¢ \"I don't know what they did inside\"\n",
        "\"\"\"\n",
        "\n",
        "DEFENDANT_SYSTEM = \"\"\"\n",
        "You are the defendant. Your job:\n",
        "- Deny or explain the accusations against you\n",
        "- Stick to facts you personally know\n",
        "- Be respectful but firm\n",
        "- Admit uncertainty where needed\n",
        "\"\"\"\n",
        "\n",
        "JUDGE_SYSTEM = \"\"\"\n",
        "You are the **Judge**. Your role:\n",
        "- Summarize briefly what each side argued\n",
        "- Decide guilt or innocence based on arguments\n",
        "- Explain your reasoning simply\n",
        "- Be neutral, fair, and concise\n",
        "\"\"\"\n",
        "\n",
        "# the two agents\n",
        "\n",
        "defense = LawyerAgent(\"Defense\", DEFENSE_SYSTEM)\n",
        "prosecution = LawyerAgent(\"Prosecution\", PROSECUTION_SYSTEM)\n",
        "\n",
        "# New agents\n",
        "plaintiff = LawyerAgent(\"Plaintiff\", PLAINTIFF_SYSTEM)\n",
        "witness = LawyerAgent(\"Witness\", WITNESS_SYSTEM)\n",
        "defendant = LawyerAgent(\"Defendant\", DEFENDANT_SYSTEM)\n",
        "judge = LawyerAgent(\"Judge\", JUDGE_SYSTEM)\n",
        "\n",
        "class TrialManager:\n",
        "    def __init__(self, agents: dict, case_background: str):\n",
        "        self.agents = agents\n",
        "        self.case = case_background\n",
        "        self.transcript = []\n",
        "\n",
        "    def record(self, speaker, message):\n",
        "        self.transcript.append(f\"{speaker.upper()}: {message}\\n\")\n",
        "\n",
        "    def run_opening_statements(self):\n",
        "        print(\"==== Opening Statements ====\\n\")\n",
        "        prosecution_msg = self.agents[\"prosecution\"].respond(f\"Opening statement to the Court. Background: {self.case}\")\n",
        "        defense_msg = self.agents[\"defense\"].respond(\"Opening statement to the Court responding to the prosecution.\")\n",
        "        print(\"PROSECUTION:\", prosecution_msg, \"\\n\")\n",
        "        print(\"DEFENSE   :\", defense_msg, \"\\n\")\n",
        "        self.record(\"prosecution\", prosecution_msg)\n",
        "        self.record(\"defense\", defense_msg)\n",
        "\n",
        "    def run_interrogation(self):\n",
        "        print(\"==== Interrogation Phase ====\\n\")\n",
        "        prose_q = self.agents[\"prosecution\"].respond(\"Interrogate the witness about what they saw.\")\n",
        "        witness_a1 = self.agents[\"witness\"].respond(prose_q)\n",
        "        prose_follow = self.agents[\"prosecution\"].respond(f\"The witness said: {witness_a1}. Respond or continue questioning.\")\n",
        "\n",
        "        defense_q = self.agents[\"defense\"].respond(\"Cross-examine the witness based on their testimony.\")\n",
        "        witness_a2 = self.agents[\"witness\"].respond(defense_q)\n",
        "        defense_follow = self.agents[\"defense\"].respond(f\"The witness said: {witness_a2}. Respond or continue questioning.\")\n",
        "\n",
        "        for role, msg in [(\"prosecution\", prose_q), (\"witness\", witness_a1),\n",
        "                          (\"prosecution\", prose_follow), (\"defense\", defense_q),\n",
        "                          (\"witness\", witness_a2), (\"defense\", defense_follow)]:\n",
        "            self.record(role, msg)\n",
        "\n",
        "    def run_closing_statements(self):\n",
        "        print(\"==== Closing Statements ====\\n\")\n",
        "        prose = self.agents[\"prosecution\"].respond(\"Give your closing statement.\")\n",
        "        defense = self.agents[\"defense\"].respond(\"Give your closing statement.\")\n",
        "        print(\"PROSECUTION:\", prose, \"\\n\")\n",
        "        print(\"DEFENSE   :\", defense, \"\\n\")\n",
        "        self.record(\"prosecution\", prose)\n",
        "        self.record(\"defense\", defense)\n",
        "\n",
        "    def judge_verdict(self):\n",
        "        print(\"==== Judge's Verdict ====\\n\")\n",
        "        max_chars = 1000  # tweak this if needed\n",
        "        summary = \"\\n\".join(self.transcript)[-max_chars:]\n",
        "        summary = summary.strip().split(\"####\")[0].strip()\n",
        "\n",
        "\n",
        "        verdict = self.agents[\"judge\"].respond(\n",
        "            f\"You are the judge. Here's the trial transcript:\\n{summary}\\nNow give your verdict and explain your reasoning.\",\n",
        "            max_new_tokens=300\n",
        "        )\n",
        "\n",
        "        print(\"JUDGE:\", verdict)\n",
        "        self.record(\"judge\", verdict)\n",
        "\n",
        "\n",
        "\n",
        "# ==== RUN THE SIMULATION ====\n",
        "\n",
        "case_background = (\n",
        "    \"The State alleges that John Doe stole proprietary algorithms from his former employer \"\n",
        "    \"and used them at a competitor. The charge is felony theft of trade secrets. \"\n",
        "    \"No physical evidence shows direct copying, but server logs indicate large downloads \"\n",
        "    \"two days before Doe resigned.\"\n",
        ")\n",
        "\n",
        "agents = {\n",
        "    \"prosecution\": prosecution,\n",
        "    \"defense\": defense,\n",
        "    \"plaintiff\": plaintiff,\n",
        "    \"witness\": witness,\n",
        "    \"defendant\": defendant,\n",
        "    \"judge\": judge,\n",
        "}\n",
        "\n",
        "trial = TrialManager(agents, case_background)\n",
        "\n",
        "trial.run_opening_statements()\n",
        "trial.run_interrogation()\n",
        "trial.run_closing_statements()\n",
        "trial.judge_verdict()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ... (keep all your existing imports and LawyerAgent class)\n",
        "\n",
        "class CourtAgent(LawyerAgent):\n",
        "    def __init__(self, name: str, role: str, case_text: str):\n",
        "        system_prompt = f\"\"\"\n",
        "        **ROLE**: {role} in this case\n",
        "        **CASE TEXT**: \"{case_text[:300]}...\"\n",
        "        **RULES**:\n",
        "        1. Respond ONLY about this specific case\n",
        "        2. Never mention other cases/laws\n",
        "        3. Keep responses under 100 words\n",
        "        4. If unsure, say \"Objection - beyond case scope\"\n",
        "        \"\"\"\n",
        "        super().__init__(name, system_prompt)\n",
        "        self.case_text = case_text\n",
        "\n",
        "# Load cases\n",
        "import pandas as pd\n",
        "from typing import List, Dict\n",
        "import csv\n",
        "cases_df = pd.read_csv(\"data.csv\")\n",
        "\n",
        "# Pick specific 10 indices (e.g., random or chosen manually to avoid first 10 always)\n",
        "selected_indices = [1, 5, 10, 25, 30, 45, 100, 150, 200, 250]\n",
        "cases_subset = cases_df.iloc[selected_indices]\n",
        "\n",
        "# Process cases\n",
        "results = []\n",
        "MAX_CASES = min(10, len(cases_df))  # Handle case if fewer than 10 cases exist\n",
        "\n",
        "for i, row in cases_df.head(MAX_CASES).iterrows():\n",
        "        case_text = row.iloc[0]\n",
        "\n",
        "        # Initialize agents (using your existing LawyerAgent class)\n",
        "        prosecutor = LawyerAgent(\"Prosecutor\",\n",
        "                               f\"You are prosecuting this case. Case details: {case_text[:300]}...\")\n",
        "        defense = LawyerAgent(\"Defense\",\n",
        "                             f\"You are defending this case. Case details: {case_text[:300]}...\")\n",
        "\n",
        "        # Generate opening statements\n",
        "        prosecution_stmt = prosecutor.respond(\"Present your opening arguments to the court\")\n",
        "        defense_stmt = defense.respond(\"Present your response to the prosecution's arguments\")\n",
        "\n",
        "        # Interrogation: Prosecution questions a witness\n",
        "        interrogation_q = prosecutor.respond(\"Ask the witness what they saw\")\n",
        "        interrogation_a = defense.respond(f\"The witness answered the prosecution: '{interrogation_q}'. Cross-examine the witness\")\n",
        "\n",
        "        judge = CourtAgent(\"Judge\", \"Judge\", case_text)\n",
        "        verdict = judge.respond(f\"Prosecutor: {prosecution_stmt} | Defense: {defense_stmt}\")\n",
        "\n",
        "        # Save results\n",
        "        results.append({\n",
        "            'case_id': i,\n",
        "            'case_preview': str(case_text)[:100] + \"...\",\n",
        "            'prosecution': prosecution_stmt,\n",
        "            'defense': defense_stmt,\n",
        "            'interrogation_question': interrogation_q,\n",
        "            'interrogation_answer': interrogation_a,\n",
        "            'verdict': verdict\n",
        "         })\n"
      ],
      "metadata": {
        "id": "tDPtQkV2Chwp",
        "outputId": "741412fe-4970-4e29-9eaa-7d5fbd031c94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "HfHubHTTPError",
          "evalue": "402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/microsoft/Phi-3-mini-4k-instruct (Request ID: Root=1-680375a2-5c965d914af7c1d80767d17a;d7482214-aa38-4a41-bb07-c46fd9c5bdc4)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/microsoft/Phi-3-mini-4k-instruct",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-cfb693eabb96>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# Generate opening statements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mprosecution_stmt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprosecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrespond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Present your opening arguments to the court\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mdefense_stmt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefense\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrespond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Present your response to the prosecution's arguments\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-55fbefa54793>\u001b[0m in \u001b[0;36mrespond\u001b[0;34m(self, user_msg, **gen_kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mgen_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"max_new_tokens\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         completion = self.client.text_generation(\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_client.py\u001b[0m in \u001b[0;36mtext_generation\u001b[0;34m(self, prompt, details, stream, model, adapter_id, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001b[0m\n\u001b[1;32m   2390\u001b[0m                     \u001b[0mwatermark\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwatermark\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2391\u001b[0m                 )\n\u001b[0;32m-> 2392\u001b[0;31m             \u001b[0mraise_text_generation_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2394\u001b[0m         \u001b[0;31m# Parse output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_common.py\u001b[0m in \u001b[0;36mraise_text_generation_error\u001b[0;34m(http_error)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0;31m# Otherwise, fallback to default error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mhttp_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_client.py\u001b[0m in \u001b[0;36mtext_generation\u001b[0;34m(self, prompt, details, stream, model, adapter_id, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001b[0m\n\u001b[1;32m   2360\u001b[0m         \u001b[0;31m# Handle errors separately for more precise error messages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2362\u001b[0;31m             \u001b[0mbytes_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_post\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2363\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2364\u001b[0m             \u001b[0mmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMODEL_KWARGS_NOT_USED_REGEX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_client.py\u001b[0m in \u001b[0;36m_inner_post\u001b[0;34m(self, request_parameters, stream)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0;31m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;31m# as well (request id and/or server error message)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHfHubHTTPError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHfHubHTTPError\u001b[0m: 402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/microsoft/Phi-3-mini-4k-instruct (Request ID: Root=1-680375a2-5c965d914af7c1d80767d17a;d7482214-aa38-4a41-bb07-c46fd9c5bdc4)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to DataFrame and save\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv('court_results.csv', index=False)\n",
        "\n",
        "# Attempt download if in Colab\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download('court_results.csv')\n",
        "except Exception as e:\n",
        "    print(\"Download not available outside Colab. Error:\", e)\n",
        "\n",
        "# Preview results\n",
        "print(\"\\nSample outputs:\")\n",
        "for r in results[:2]:\n",
        "    print(f\"\\nCase {r['case_id']}: {r['case_preview']}\")\n",
        "    print(f\"Prosecution: {r['prosecution'][:100]}...\")\n",
        "    print(f\"Defense: {r['defense'][:100]}...\")\n",
        "\n",
        "print(f\"\\nðŸŽ‰ Saved {len(results_df)} case results to court_results.csv\")\n",
        "print(\"Columns:\", results_df.columns.tolist())\n"
      ],
      "metadata": {
        "id": "QijaeP9PQ1ZO",
        "outputId": "0887234f-7b9e-46cb-ab49-fef09da50475",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4496c31b-5a3c-44fb-b6cd-92e3666412d0\", \"court_results.csv\", 55375)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample outputs:\n",
            "\n",
            "Case 0: \" F. NARIMAN  J. Leave granted. In 2008  the Punjab State Water Supply Sewerage Board  Bhatinda issu...\n",
            "Prosecution: Your Honor, ladies and gentlemen of the court, we are here today to discuss a matter of utmost impor...\n",
            "Defense: Your Honor, our defense is based on the fact that our client, F. Nariman, was granted leave to defen...\n",
            "\n",
            "Case 1: \" F. NARIMAN  J. Leave granted. In 2008  the Punjab State Water Supply Sewerage Board  Bhatinda issu...\n",
            "Prosecution: Your Honor, ladies and gentlemen of the court, we are here today to discuss a matter of utmost impor...\n",
            "Defense: Your Honor, our defense is based on the fact that our client, F. Nariman, was granted leave to defen...\n",
            "\n",
            "ðŸŽ‰ Saved 10 case results to court_results.csv\n",
            "Columns: ['case_id', 'case_preview', 'prosecution', 'defense', 'interrogation_question', 'interrogation_answer']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nsf2CdNePMF-"
      }
    }
  ]
}